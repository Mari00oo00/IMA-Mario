# 机器学习

## 模型的评估与选择

### **引入**

背景：在m个样本中有a个样本分类错误

错误率：$$E=a/m$$，精度：$$1-a/m$$

**训练误差（经验误差）**

学习器在训练集上的误差

**泛化误差**

学习器在新样本上的误差。泛化误差不可避免

**过拟合**

机器把训练集学习得太好从而导致泛化性能下降

**欠拟合**

机器对训练样本的一般性质尚未学好

**数据集**

$$D=\{x_1,x_2,...,x_m \}$$表示包含m个示例的数据集

$$x_i=(x_{i1};x_{i2},...;x_{id})$$是d维样本空间$$\chi$$的一个向量，其中$$x_{ij}$$是$$x_{i}$$在第$$j$$个属性上的取值

**样例与标记**

样例：拥有了标记信息的示例。一般地，用$$(x_i,y_i)$$表示第$$i$$个样例，其中$$y_i \in \gamma$$是示例$$x_i$$的标记，$$\gamma$$是所有标记的集合，亦称“标记空间”或“输出空间”

**学习任务**（详见西瓜书p3）

根据欲预测的值的类型：

* 分类：欲预测的是离散值
* 回归：欲预测的是连续值

根据涉及到的类别：

* “二分类”任务：只涉及两个类别。通常一个类为“正类”，另一个为“反类”
* “多分类”任务

根据训练数据是否拥有标记信息：

* 监督学习（分类和回归是其中的代表）
* 无监督学习（聚类是其中的代表）

### 评估方法

1. 通过测试来评估学习器的泛化误差：

   * 使用测试集，但测试集与训练集的关系应该尽可能小。

   只有一个数据集时产生训练集和测试集的方法：

   1. 留出法

   2. **交叉验证法（$$k$$折交叉验证）：**

      * [交叉验证法与留出法及其python实现](https://blog.csdn.net/weixin_43216017/article/details/86692358?ops_request_misc=%7B%22request%5Fid%22%3A%22164282550316780271560942%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164282550316780271560942&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-86692358.first_rank_v2_pc_rank_v29&utm_term=留一法交叉验证python&spm=1018.2226.3001.4187)

      * 将数据集$$D$$划分成$$k$$个大小相似的互斥子集，每次用$$k-1$$个子集的并集作为训练集，余下的子集是测试集，这样就可以进行$$k$$次训练与测试，最终返回的是这$$k$$个测试结果的均值
      * 留一法（数据集较大时慎用）：令$$k=样本数$$

   3. 自助法

   4. 调参与最终模型

2. 性能度量

   **回归任务中最常用得性能度量：均方误差**（$$MSE$$）（见西瓜书29页）

   $$E(f;D)=\frac {1}{m}\sum\limits_{i=1}^m (f(x_i)-y_i)^2$$

   **分类任务中常用的性能度量：**

   * 错误率与精度

   * 查准率、查全率与F1

   * ROC与AUC

   * 代价敏感错误率与代价曲线

3. 比较检验

   * 假设检验
   * 交叉验证t检验
   * McNemar检验
   * Friedman检验与Nemenyi后续检验

4. 偏差与方差

## 线性模型

**基本形式**

$$f(x)=w^Tx+b$$，其中$$w=(w_1;w_2;...;w_d)$$，$$x=(x_1;x_2;...;x_d)$$（即含有d个属性）。目的是学得$$w$$和$$b$$

### 线性回归

#### 正规方程法

**能直接求出解析解，因此不用训练多次！**

最小二乘法：基于均方误差最小化来进行模型求解的方法

**前置设定：**

数据集：$$D=\{ (x_1,y_1)，(x_2，y_2),...,(x_n,y_n) \}$$

$$X=(x_1,x_2,...,x_n)^T=\begin{pmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{pmatrix}$$，$$Y=\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} $$，其中$$x_i \in R^p$$，$$y_i \in R$$，$$w=\begin{pmatrix}   w_1 \\ w_2 \\ \vdots \\ w_n\end{pmatrix}$$

设$$f(w)=w^Tx$$

推导：
$$
\begin{aligned}
L(w)&=\sum\limits_{i=1}^m||w^Tx_i-y_i||^2 \qquad 最小二乘法\\&=\sum\limits_{i=1}^m(w^Tx_i-y_i)^2 \\&=[w^Tx_1-y_1 \quad w^Tx_2-y_2 \quad...\quad w^Tx_n-y_n] \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=([w^Tx_1 \quad w^Tx_2 \quad...\quad w^Tx_n]-[y_1 \quad y_2 \quad...\quad y_n]) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^T[x_1 \quad x_2 \quad...\quad x_n]-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T)(Xw-Y) \qquad 从第三步可以看出右边的矩阵是左边矩阵的逆置 \\&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY \qquad 顺序不能写错\\&=w^TX^TXw-2w^TX^TY+Y^TY \qquad上一步的中间两项互为逆置，因此结果相同（都为同一个数），可以合并
\end{aligned}
$$
所以，$$\hat{w}=arg\min L(w)$$.

> arg   是变元（即自变量argument）的英文缩写。
> arg min 就是使后面这个式子达到最小值时的变量的取值
> arg max 就是使后面这个式子达到最大值时的变量的取值

求最小值的方法是对$$w$$求导，并令式子为0，求出此时的$$w$$：
$$
\begin{aligned}
&\frac{\partial L(w)}{\partial w}=2X^TXw-2X^TY=0 \\&\therefore X^TXw=X^TY \\&\therefore w=(X^TX)^{-1}X^TY
\end{aligned}
$$

##### **代码实现：**

**主要步骤：**导入数据集-查看数据集类型-转换数据集为多维矩阵类型（如需转置则转置）-定义正规方程的函数并经此将$$w$$求出-分别训练自己的模型与sklearn的模型-比较二者最终的损失函数值

**sklearn入门：**

[非常详细的sklearn介绍](https://blog.csdn.net/algorithmPro/article/details/103045824?ops_request_misc=%7B%22request%5Fid%22%3A%22164272427916780274136815%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164272427916780274136815&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-103045824.first_rank_v2_pc_rank_v29&utm_term=sklearn&spm=1018.2226.3001.4187)

[极简Scikit-Learn入门](https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2648929856&idx=1&sn=0a6ce240f46bea7d74f2a850625de832&chksm=8794e46ab0e36d7c449579513e69fbbc957ac61eac10c6f863ed4a860debd3dca1ed0ba442c2&token=2004915986&lang=en_US#rd)

[机器学习神器：sklearn的快速使用](https://juejin.cn/post/6844903632576446478)

**Linear Regression中的参数及功能：**

[线性模型之Linear Regression_walk_power](https://blog.csdn.net/walk_power/article/details/82924363?ops_request_misc=%7B%22request%5Fid%22%3A%22164278046516780261964167%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164278046516780261964167&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-82924363.first_rank_v2_pc_rank_v29&utm_term=linearregression.fit函数&spm=1018.2226.3001.4187)

**糖尿病数据集介绍：**

官方（英文）：[7.1. Toy datasets — scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)

中文：[sklearn数据集之“糖尿病病人体检数据集”](https://blog.csdn.net/Zee_Chao/article/details/95884401?ops_request_misc=%7B%22request%5Fid%22%3A%22164275178816780357286281%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164275178816780357286281&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-95884401.first_rank_v2_pc_rank_v29&utm_term=sklearn糖尿病数据集&spm=1018.2226.3001.4187)

**矩阵的运算：**

[矩阵的创建与运算](https://blog.csdn.net/m0_37759590/article/details/105160374?ops_request_misc=%7B%22request%5Fid%22%3A%22164274088716780271983059%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164274088716780271983059&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105160374.first_rank_v2_pc_rank_v29&utm_term=python+mat()&spm=1018.2226.3001.4187)

**损失函数相关知识：**

[均方误差，log loss和交叉熵](https://blog.csdn.net/weixin_38410551/article/details/104973011?ops_request_misc=%7B%22request%5Fid%22%3A%22164277875816780271580985%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164277875816780271580985&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-104973011.first_rank_v2_pc_rank_v29&utm_term=均方误差损失函数&spm=1018.2226.3001.4187)

**pytorch的运用与torch-cpu的安装：**

[深度学习环境配置5——windows下的torch-cpu=1.2.0环境配置](https://blog.csdn.net/weixin_44791964/article/details/120655098)

`HTTPError: HTTP 000 CONNECTION FAILED`错误的解决办法：[在终端通过conda指令创建虚拟python环境时，出现CondaHTTPError: HTTP 000 CONNECTION FAILED等错误](https://blog.csdn.net/kimpa0723/article/details/118912484?ops_request_misc=%7B%22request%5Fid%22%3A%22164286290616780255269742%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=164286290616780255269742&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-118912484.first_rank_v2_pc_rank_v29&utm_term=CondaHTTPError%3A&spm=1018.2226.3001.4187)

[PyTorch中文文档 ](https://pytorch-cn.readthedocs.io/zh/latest/)

**`LeaveOneOut()`相关：**

官方文档：[sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leaveoneout#sklearn.model_selection.LeaveOneOut)

**在矩阵（多维数组）前加上一列：**

[np.hstack 用法](https://blog.csdn.net/G66565906/article/details/84142034?ops_request_misc=%7B%22request%5Fid%22%3A%22164292971316780269822279%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164292971316780269822279&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-84142034.first_rank_v2_pc_rank_v29&utm_term=np.hstack&spm=1018.2226.3001.4187)

[Python中的numpy.ones()](https://blog.csdn.net/cunchi4221/article/details/107471968?ops_request_misc=%7B%22request%5Fid%22%3A%22164292924816781683974723%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164292924816781683974723&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107471968.first_rank_v2_pc_rank_v29&utm_term=np.ones&spm=1018.2226.3001.4187)

**设定与说明：**

该数据集的类型为sklearn.utils.Bunch（本质是字典），详见[sklearn.utils.Bunch的属性](https://blog.csdn.net/lly1122334/article/details/88741971?ops_request_misc=%7B%22request%5Fid%22%3A%22164275423516780264093759%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164275423516780264093759&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-88741971.first_rank_v2_pc_rank_v29&utm_term=sklearn.utils.Bunch&spm=1018.2226.3001.4187)

用`data`来装数据集

sklearn中的留一法函数的运用（基本仿照官方示例打的）：

*注：`split()`返回的只是分组之后的组的索引值*

python里对向量进行次方的操作是对每一个元素单独进行次方，而不是点乘。

* 求预测值从而求$$MSE$$用的是向量的方法直接求出一个整体：`(np.array(X_train*wT.T-Y_train)**2).sum()/441`（不先转成矩阵再平方会报错，因为是非方阵）

注：该方法用$$Xw$$而不是$$w^Tx$$

```python
loo = LeaveOneOut()
for train_index, test_index in loo.split(X):
    print("train_index:", train_index, "test_index:", test_index)
    X_train = X[train_index]
    Y_train = Y[train_index]
# 训练集的录入
    X_test = X[test_index]
    Y_test = Y[test_index]
# 测试集的录入
```

* 需在$$X$$的前面加上一列$$x_0=1$$，不然$$MSE$$的值会很大

```python
X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
```

* sklearn中的线性回归函数的调用：

```python
lr_fun = LinearRegression()
lr_fun.fit(np.array(X_train), np.array(Y_train))
# X与Y的输入
y_pre_sk = lr_fun.predict(np.array(X_train))
print("sk_loss:", (np.array(y_pre_sk-Y_train)**2).sum()/441)
```

全部代码：

```python
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import LeaveOneOut
# 导入留一法函数
data = datasets.load_diabetes()
# 导入糖尿病数据集


def linear_regression(X, Y):
    """
    求wT
    """
    XT = X.T
    XTX = XT*X
    w = XTX.I*XT*Y
    return w


X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
# 这里不用转置

Y = data.target
Y = np.mat(Y).T

loo = LeaveOneOut()
for train_index, test_index in loo.split(X):
    X_train = X[train_index]
    Y_train = Y[train_index]
    X_test = X[test_index]
    Y_test = Y[test_index]

w = linear_regression(X_train, Y_train)
print("loss:", (np.array(X_train*w-Y_train)**2).sum()/441)

lr_fun = LinearRegression()
lr_fun.fit(np.array(X_train), np.array(Y_train))
# X与Y的输入
y_pre_sk = lr_fun.predict(np.array(X_train))
print("sk_loss:", (np.array(y_pre_sk-Y_train)**2).sum()/441)
```

#### 梯度下降法

[什么是梯度下降法？](https://www.zhihu.com/question/305638940/answer/1639782992)

[梯度下降法及其Python实现](https://blog.csdn.net/yhao2014/article/details/51554910?ops_request_misc=&request_id=&biz_id=102&utm_term=梯度下降法python实现&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-51554910.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187)

[机器学习之梯度下降算法(python实现)](https://blog.csdn.net/weixin_43872709/article/details/108938376?ops_request_misc=%7B%22request%5Fid%22%3A%22164301014616780264092110%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164301014616780264092110&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-108938376.first_rank_v2_pc_rank_v29&utm_term=梯度下降法python实现&spm=1018.2226.3001.4187)

**公式：**$$\theta_j:=\theta_j-\alpha \frac{\partial J(\theta_0,\theta_1...\theta_n)}{\partial \theta_j}$$，其中$$j=0,1,...,n$$；$$\alpha$$被称为学习率，是个正数；$$:=$$是赋值的意思；要同时更新$$\theta_0,\theta_1,...,\theta_n$$的坐标

**原理：**一步一步逼近局部最优解（导数为0的点。线性方程只有1个全局最优解），且步长随着迭代的次数增加而降低。

##### 用梯度下降法实现线性回归

**前置设定于说明：**

数据集有$$m$$个样本，每个样本有n个特征

**假设函数（即要求的函数）：**$$h_\theta(x)=\theta^Tx=\theta_0+\theta_1x_1+\cdots+\theta_nx_n=\sum\limits_{i=0}^n \theta_i x_i$$，其中$$x^{(i)}_j$$意为第$$i$$个样本的第$$j$$个特征，规定$$x^0_j=1$$（即在矩阵前添加一列数字1）

**代价函数（损失函数）：**$$J(\theta_0,\theta_1,\cdots,\theta_n)=J(\theta)=\frac{1}{2m} \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{1}{2m}(X\theta-Y)^2向量每个值相加$$（这里乘1/2是为了下一步求偏导数时式子更美观）

**所以**$$\theta_j:=\theta_j-\alpha \frac{\partial J(\theta_0,\theta_1...\theta_n)}{\partial \theta_j}=\theta_j-\alpha \cdot \frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$$（具体过程很容易推导出来）

**把$$\theta_j$$一次性全部求出来（即写成向量形式$$\theta$$）：**

规定：$$\theta$$为$$(n+1)\cdot1$$的列向量，$$X$$为$$m(n+1)$$的矩阵，$$Y$$为$$m\cdot1$$的列向量
$$
\begin{aligned}
\theta:=\theta-\frac{\alpha}{m}((X \theta-Y)^T \cdot X)^T=\theta- \frac{\alpha}{m}X^T(X \theta-Y)
\end{aligned}
$$
推导过程：
$$
\begin{aligned}
&\because \theta为(n+1)\cdot1的列向量\\&
\therefore\frac{\alpha}{m}应该乘以一个同样为(n+1)\cdot1的列向量\\&
\therefore再看右边，要把\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j变成一个(n+1)\cdot1的列向量或1\cdot(n+1)的行向量\\&
又\because通过观察可得x^{(i)}_j乘(h_\theta(x^{(i)})-y^{(i)})时可看成一个列向量乘(h_\theta(x^{(i)})-y^{(i)})，且结果要为上一步所说\\&
\therefore\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j的结果可以看成(h_\theta(x^{(i)})-y^{(i)})\cdot X\\&
又\because X的行数为m\\&
\therefore(h_\theta(x^{(i)})-y^{(i)})要写成列数为m的行向量\\&
又\because Y为m\cdot 1的列向量\\&
\therefore h_\theta(x^{(i)})可写成X\theta\\&
\therefore (X\cdot \theta -Y)为(1\cdot m)的行向量\\&
\therefore (X \theta-Y)^T \cdot X为1\cdot (n+1)的行向量\\&
\therefore最后转置一下即可
\end{aligned}
$$
也可以对代价函数求导后再算（注意好变换size就行）

<img src="http://mari0.oss-cn-guangzhou.aliyuncs.com/img/image-20220124224813833.png" alt="image-20220124224813833" style="zoom: 80%;" />

###### 代码实现：

**过程：**取出$$X$$与$$Y$$，并对$$X$$增加一列数字1-设置学习率与训练次数，把$$\theta$$全设置为0-迭代并求loss-不断调参使loss尽可能小

**设定与相关说明：**

规定$$\theta$$一开始全为0，且为列向量；学习率为1，次数50000次

```python
theta = np.zeros(11).T
alpha = 1
times = 50000
```

得到梯度的函数

```python
def get_theta(X, Y, theta, alpha, m):
    theta = theta-(alpha/m)*(X.T*(X*theta-Y))
    return theta
```

损失函数

```python
def get_loss(X, Y, theta):
    loss = (1/(2*len(X)))*(np.array((X*theta-Y))**2)
    return loss.sum()
```

迭代

```python
for i in range(times):
    theta = get_theta(X, Y, theta, alpha, len(X))
    loss = get_loss(X, Y, theta)
    print(loss)
```

全部代码：

```python
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import LeaveOneOut
# 导入留一法函数
data = datasets.load_diabetes()
# 导入糖尿病数据集

X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
# 这里不用转置

Y = data.target
Y = np.mat(Y).T

theta = np.zeros(11)
theta = np.mat(theta).T

alpha = 1
times = 50000


def get_loss(X, Y, theta):
    loss = (1/(2*len(X)))*(np.array((X*theta-Y))**2)
    return loss.sum()


def get_theta(X, Y, theta, alpha, m):
    theta = theta-(alpha/m)*(X.T*(X*theta-Y))
    return theta


for i in range(times):
    theta = get_theta(X, Y, theta, alpha, len(X))
    loss = get_loss(X, Y, theta)
    print(loss)
```

### 逻辑回归

[用人话讲明白逻辑回归Logistic regression](https://zhuanlan.zhihu.com/p/139122386)

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

#### 引入（以二分类任务为例）

**因变量：**分类型，一般取0或1。因此，$$y=w^Tx$$不适用。

**Sigmoid函数（Logistic函数）：**

* 原始公式：$$S(x)=\frac{1}{1+e^{-x}}$$，变为多元函数后：$$P(Y=1)=\frac{1}{1+e^{-X\theta}}$$
* 意义：将因变量映射在$$(0,1)$$之间
* 图像：![image-20220125154313415](http://mari0.oss-cn-guangzhou.aliyuncs.com/img/image-20220125154313415.png)

**似然函数与极大似然估计法：**

* 似然函数：

  [似然（likelihood）和概率（probability）的区别与联系](https://blog.csdn.net/songyu0120/article/details/85059149)

  似然函数：$$L(\theta|x)$$，概率：$$P(x|\theta)$$，二者的值相等。通过数据求$$\theta$$的过程即训练阶段。

* 极大似然估计法：

  设定：有$$N$$个样本

  1. 单个事件正确的概率：$$P(y|x,\theta)=p(y=1|x,\theta)^{y}\cdot p(y=0|x,\theta)^{1-y}$$
  2. 多个事件正确的概率：$$P(Y|X,\theta)=\prod \limits_{i=1}^N p(y^{(i)}|x^{(i)},\theta)$$

  根据$$L(\theta|x)=P(x|\theta)$$，可得极大似然估计函数为：

  $$\hat{\theta}=arg\ max\ P(Y|X,\theta)$$

  为了方便运算，要把连乘换成连加，因此要取对数运算。具体推导过程如下：
  $$
  \begin{aligned}
  \hat{\theta}&=arg\ max\ P(Y|X,\theta) \\&=arg\ max\ log(P(Y|X,\theta)) \\&=arg\ max\ log(\prod \limits_{i=1}^N p(y^{(i)}|x^{(i)},\theta)) \quad 事件之间相互独立，可以用乘法 \\&=arg\ max\ log(\prod \limits_{i=1}^N p(y^{(i)}=1|x^{(i)},\theta)^{y^{(i)}}\cdot p(y^{(i)}=0|x^{(i)},\theta)^{1-y^{(i)}}) \\&=arg\ max\ log(\prod \limits_{i=1}^N p(y^{(i)}=1|x^{(i)},\theta)^{y^{(i)}}\prod \limits_{i=1}^N p(y^{(i)}=0|x^{(i)},\theta)^{1-y^{(i)}}) \\&=arg\ max\ \sum \limits_{i=1}^N [ log(p(y^{(i)}=1|x^{(i)},\theta)^{y^{(i)}}+log\ (p(y^{(i)}=0|x^{(i)},\theta)^{1-y^{(i)}}))] \\&=arg\ max\ \sum \limits_{i=1}^N [log(\frac{1}{1+e^{-\theta ^Tx^{(i)}}})^{y^{(i)}}+ log(\frac {e^{-\theta ^Tx^{(i)}}}{1+e^{-\theta ^Tx^{(i)}}})^{1-y^{(i)}}] \\&=arg\ max\ \sum \limits_{i=1}^N [(y^{(i)})log(\frac{1}{1+e^{-\theta ^Tx^{(i)}}})+(1-y^{(i)})log(\frac {e^{-\theta ^Tx^{(i)}}}{1+e^{-\theta ^Tx^{(i)}}})]
  \end{aligned}
  $$
  

**损失函数：**在逻辑回归模型中，**最大化似然函数**和**最小化损失函数**实际上是等价的，因此损失函数为（最后可以除以样本数）：

$$J(\theta)=-log\ L(\theta)=-\sum \limits_{i=1}^N [(y^{(i)})log(\frac{1}{1+e^{-\theta ^Tx^{(i)}}})+(1-y^{(i)})log(\frac {e^{-\theta ^Tx^{(i)}}}{1+e^{-\theta ^Tx^{(i)}}})]$$

**梯度下降法求解$$\theta$$：**

从简单开始，对sigmoid函数$$S(x)=\frac{1}{1+e^{-x}}$$求导：$$S'(x)=\frac{e^{-x}}{(1+e^{-x})^2}$$，通过观察得$$S'(x)=S(x)\cdot (1-S(x))$$

按照以往的习惯，我们还是用梯度下降法求损失函数的最小值。

令$$f(x^{(i)},\theta)=log(\frac{1}{1+e^{-\theta ^Tx^{(i)}}})$$,$$g(y^{(i)},x^{(i)},\theta)=[(y^{(i)})log(\frac{1}{1+e^{-\theta ^Tx^{(i)}}})+(1-y^{(i)})log(\frac {e^{-\theta ^Tx^{(i)}}}{1+e^{-\theta ^Tx^{(i)}}})]$$，

因此$$J(\theta)=-\sum \limits_{i=1}^N [(y^{(i)})log(f(x^{(i)}，\theta))+(1-y^{(i)})log(1-f(x^{(i)}，\theta)] =-\sum \limits_{i=1}^N g(y^{(i)},x^{(i)},\theta)$$

下面对$$g(y^{(i)},x^{(i)},\theta)$$求$$\theta_j$$的偏导：
$$
\begin{aligned}
&\because \frac{\partial f(x^{(i)},\theta)}{\partial\theta_j}=f(x^{(i)},\theta)\cdot (1-f(x^{(i)},\theta))\cdot x^{(i)}_j\\&
\therefore \frac{\partial g(y^{(i)},x^{(i)},\theta)}{\partial \theta_j}=y^{(i)}\frac{1}{f(x^{(i)},\theta)}\frac{\partial f(x^{(i)},\theta)}{\partial \theta_j}-(1-y^{(i)})\frac{1}{1-f(x^{(i)},\theta)}\frac{\partial f(x^{(i)},\theta)}{\partial \theta_j}\\&
=(\frac{y^{(i)}}{f(x^{(i)},\theta)}-\frac{1-y^{(i)}}{1-f(x^{(i)},\theta)})\cdot \frac{\partial f(x^{(i)},\theta)}{\partial \theta_j}\\&
=(\frac{y^{(i)}}{f(x^{(i)},\theta)}-\frac{1-y^{(i)}}{1-f(x^{(i)},\theta)})\cdot f(x^{(i)},\theta)\cdot (1-f(x^{(i)},\theta))\cdot x^{(i)}_j\\&
=[y^{(i)}(1-f(x^{(i)},\theta))-(1-y^{(i)})f(x^{(i)},\theta)]\cdot x^{(i)}_j\\&
=(y-f(x^{(i)},\theta))\cdot x^{(i)}_j
\end{aligned}
$$
因此，$$\frac{\partial J(\theta)}{\partial \theta_j}=-\sum \limits_{i=1}^N (y-f(x^{(i)},\theta))\cdot x^{(i)}_j=\sum \limits_{i=1}^N (\frac{1}{1+e^{-\theta^T x^{(i)}}}-y^{(i)})\cdot x^{(i)}_j$$

根据$$\theta_j:=\theta_j-\alpha \frac{\partial J(\theta_0,\theta_1...\theta_n)}{\partial \theta_j}=\theta_j-\alpha \cdot \sum \limits_{i=1}^N (\frac{1}{1+e^{-\theta^T x^{(i)}}}-y^{(i)})\cdot x^{(i)}_j$$

和$$\theta:=\theta-\frac{\alpha}{N}((X \theta-Y)^T \cdot X)^T=\theta- \frac{\alpha}{N}X^T(X \theta-Y)$$两条公式可以推导出逻辑回归的$$\theta$$
$$
\begin{aligned}
\theta:=\theta- \alpha X^T(\frac{1}{1+e^{-X \theta}}-Y)
\end{aligned}
$$

**查全率与查准率：**

由于逻辑回归是分类任务，因此这里我们用查全率和查准率作为模型的衡量指标

| 真实情况 |    预测    |    结果    |
| :------- | :--------: | :--------: |
|          |    正例    |    反例    |
| 正例     | TP(真正例) | FN(假反例) |
| 反例     | FP(假正例) | TN(真反例) |

查准率（$$P$$）：$$P=\frac{TP}{TP+FP}$$

查全率（$$R$$）:$$R=\frac{TP}{TP+FN}$$

#### 代码实现：

**过程：**设置学习率（这里设为0.01，否则预测值会很大）和训练次数--求sigmoid函数列向量并以之来求θ--每一轮都用θ求估计值--用最后一组估计值来求查准率和查全率

[逻辑回归python实现](https://blog.csdn.net/qq_37667364/article/details/81532339?ops_request_misc=&request_id=&biz_id=102&utm_term=逻辑回归python&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-81532339.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187)

[逻辑回归模型（Logistic Regression）及Python实现](https://blog.csdn.net/wang603603/article/details/81334218?ops_request_misc=&request_id=&biz_id=102&utm_term=逻辑回归python&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-5-81334218.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187)

将$$\frac{1}{1+e^{-X \theta}}$$变为列向量，$$\frac{1}{1+e^{-\theta^T x^{(i)}}}$$便是其中的元素：

```python
def get_sigmoid_matrix():
    sigmoid_matrix = []
    for i in range(len(X)):
        sigmoid_matrix.append(1+math.exp((-X*theta)[i]))
    return np.mat(sigmoid_matrix).T
```

求θ的函数：

```python
def get_theta(X, Y, theta, alpha):
    theta = theta-alpha*X.T*(get_sigmoid_matrix(theta)-Y)
    return theta
```

求估计值，并生成列向量（规定>=0.5为1，否则为0）：

```python
def get_predict(theta):
    pre_matrix = get_sigmoid_matrix(theta)
    for i in range(m):
        if pre_matrix[i] < 0.5:
            pre_matrix[i] = 0
        else:
            pre_matrix[i] = 1
    return pre_matrix
```

求查全率和查准率：

```python
def precision_and_recall_rate(Y_pre, Y):
    TP = 0 #真正例
    FN = 0 #假反例
    FP = 0 #假正例
    TN = 0 #真反例
    for i in range(m):
        if Y_pre[i] == 1 and Y[i] == 1:
            TP += 1
        if Y_pre[i] == 1 and Y[i] == 0:
            FP += 1
        if Y_pre[i] == 0 and Y[i] == 1:
            FN += 1
        if Y_pre[i] == 0 and Y[i] == 0:
            TN += 1
    precision_rate = TP/(TP+FP)
    recall_rate = TP/(TP+FN)
    return precision_rate, recall_rate
```

全部代码：

```python
import math
import numpy as np
from sklearn import datasets
# 导入留一法函数
data = datasets.load_diabetes()
# 导入糖尿病数据集

X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
# 这里不用转置

Y = data.target
Y = np.mat(Y).T
# 由于该数据集并不适合用于逻辑回归，因此就根据标签的取值范围随便设了一个指标
for i in range(len(X)):
    if Y[i] < 150:
        Y[i] = 0
    else:
        Y[i] = 1


theta = np.zeros(11)
theta = np.mat(theta).T
alpha = 0.01
times = 5000
m = len(X)


def get_sigmoid_matrix(theta):
    sigmoid_matrix = []
    for i in range(m):
        sigmoid_matrix.append(1/(1+math.exp(np.array(-(X*theta)[i]))))
    return np.mat(sigmoid_matrix).T


def get_theta(X, Y, theta, alpha):
    theta = theta-alpha*X.T*(get_sigmoid_matrix(theta)-Y)
    return theta


def get_predict(theta):
    pre_matrix = get_sigmoid_matrix(theta)
    for i in range(m):
        if pre_matrix[i] < 0.5:
            pre_matrix[i] = 0
        else:
            pre_matrix[i] = 1
    return pre_matrix


def precision_and_recall_rate(Y_pre, Y):
    TP = 0 #真正例
    FN = 0 #假反例
    FP = 0 #假正例
    TN = 0 #真反例
    for i in range(m):
        if Y_pre[i] == 1 and Y[i] == 1:
            TP += 1
        if Y_pre[i] == 1 and Y[i] == 0:
            FP += 1
        if Y_pre[i] == 0 and Y[i] == 1:
            FN += 1
        if Y_pre[i] == 0 and Y[i] == 0:
            TN += 1
    precision_rate = TP/(TP+FP)
    recall_rate = TP/(TP+FN)
    return precision_rate, recall_rate


for i in range(500):
    theta = get_theta(X, Y, theta, alpha)
    pre_matrix = get_predict(theta)


print(pre_matrix)
precision_rate, recall_rate = precision_and_recall_rate(pre_matrix, Y)
print(precision_rate)
print(recall_rate)
```

最终结果：

查全率(Recall rate)：0.71568

查准率(precision_rate)：0.73

## 决策树

### 基本概念

决策树有时指一种机器学习方法（以二分类任务为例，判定或决策当前样本是否属于正类），有时指学得的树。

决策树的组成：一个根结点、若干个内部结点和若干个叶结点。

根结点包含样本全集。

叶结点对应于决策的结果，非叶结点的其它结点对应于一个属性测试。

*（内部结点表示一个特征或属性，叶结点表示一个类。图详见《统计学习方法》P68）*

如图不难看出，决策树的生成是一个递归的过程，即当无法再划分出新的结点时就会返回。

**返回的三种情况：**

1. 当前结点包含的样本全属于同一类别，无需划分

2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分（于是我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别）

3. 当前结点包含的样本集为空，不能划分（于是我们把当前结点标记为叶结点，并将其类别设定为其父结点所含样本最多的类别）

*注意：情形2是在利用当前结点的后验分布，而情形3则是把父结点的样本分布作为当前结点的先验分布*

**最优划分属性：**在划分的过程中，在属性集中找到一个属性使得决策树的分支结点所包含的样本尽可能属于同一类别（即结点的纯度越来越高）

### 基本步骤及三种算法

基本步骤：特征选择、决策树的生成（考虑局部最优）和决策树的修剪（考虑全局最优）

三种算法：**ID3算法**、**C4.5算法**和**CART算法**

#### 特征选择

含义：决定用哪个特征来划分特征空间

两种准则：信息增益和信息增益比

##### 信息增益

[信息增益到底怎么理解呢？](https://www.zhihu.com/question/22104055/answer/161415527)

**信息熵：**

**含义：**表示随机变量不确定性的度量。（熵越高，不确定性越大，信息量越大）

设$$X$$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i,\ i=1,2,\cdots,n$$

则$$X$$的熵定义为$$H(X)=-\sum \limits_{i=1}^n p_i log\ p_i$$，规定$$0log0$$为$$0$$

当对数以$$2$$或$$e$$为底时，这时熵的单位分别被称为*比特*或*纳特*（当$$X$$只取0，1时，以2为底）

* 条件熵

  设有随机变量$$Y$$，其与$$X$$联合概率分布为$$P(X=x_i,Y=y_i)=p_{ij},\ i=1,2,\cdots,n;\ j=1,2,\cdots,m$$

  条件熵定义为$$X$$给定条件下$$Y$$的条件概率分布的熵对$$X$$的数学期望：

  $$H(Y|X)=\sum \limits_{i=1}^n p_iH(Y|X=x_i)$$，其中$$p_i=P(X=x_i)$$

  *(用最简单的理解，即把$$Y$$看成正反类的集合，那么$$H(Y|X=x_i)=-\sum\limits_{k=1}^2 p_{(x_i,Y_k)}log\ p_{(x_i,Y_k)}$$，即只与类的数目有关)*

* 经验熵与经验条件熵

  当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时对应的熵和条件熵

**互信息：**熵与条件熵之差

针对不同的特征，一个数据集可产生多个信息增益（即多个互信息），表示特定特征对数据集分类的不确定性减少的程度。因此，信息增益大的特征具有更强的分类能力。

设数据集$$D$$有$$K$$个类（如正反类）$$C_k,k=1,2,\cdots,K$$，特征$$A$$有$$n$$个不同的取值，其将$$D$$分成$$n$$个不同的子集。用$$|D|$$表示该数据集的容量（样本数），$$|C_k|$$表示$$k$$类所含的样本数，$$|D_i|,i=1,2,\cdots,n$$表示用特征$$A$$分成的子集$$D_i$$所含的样本数。记子集$$D_i$$中属于$$C_k$$的样本集合为$$D_{ik}$$。

则$$H(D)=-\sum \limits_{k=1}^K \frac{|C_k|}{|D|}log_2\ \frac{|C_k|}{|D|}$$

$$H(D|A)=\sum\limits_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum\limits_{i=1}^n \frac{|D_i|}{|D|}(\sum\limits_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2\ \frac{|D_{ik}|}{|D_i|})$$

则特征$$A$$对数据集$$D$$的信息增益为

$$g(D,A)=H(D)-H(D|A)$$

##### 信息增益比

信息增益存在偏向于选择值较多的特征之问题，信息增益比能对该问题进行矫正

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中$$H_A(D)=-\sum\limits_{i=1}^n \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$，意为训练集$$D$$关于特征$$A$$的值的熵

#### ID3算法

特点：在分结点的过程中不断选择信息增益最大的特征

##### 代码实现

[python实现ID3决策树分类算法](https://blog.csdn.net/colourful_sky/article/details/82056125?ops_request_misc=%7B%22request%5Fid%22%3A%22164421959316780271597530%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164421959316780271597530&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82056125.first_rank_v2_pc_rank_v29&utm_term=id3算法python实现&spm=1018.2226.3001.4187)

* 规定：
  * 每个传进函数的DataFrame或列表一定要先排序好
  * DataFrame的列索引用数字表示
  * 在不画图的情况下，决策树用字典的嵌套呈现

**导入信息**

报错：*“UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb1 in position 0: invalid start byte”*

原因：文本中含有汉字，汉字的编码格式是“gbk”、“gb2312”或“gb18030”，因此要在导入时改变编码格式

```python
data = pd.read_csv("melon.txt", encoding='gbk')  # 格式为DataFrame
del data['编号']
feature_name = data.columns.tolist()  # 提取列索引作为特征集
del feature_name[-1]  # 删掉类别列
column_number = data.shape[1]  # 获取列数
data.columns = range(column_number)  # 重设列索引为数字形式
```

**信息处理**

```python
category_sample_number = data[column_number-1].value_counts()  # 提取各类所含的样本数
number_category = len(data.iloc[:, -1].value_counts())  # 类的种类数目
```

**计算熵的函数**

```python
def get_entropy(number, category, data_set):
    """
    :param number: 类的种类个数
    :param category: 包含各类种类或特征各取值种类个数的列表
    :param data_set: 数据集
    :return: 数据集的熵
    """
    entropy = 0
    for i in range(number):
        entropy += category[i]/len(data_set)*math.log(category[i]/len(data_set), 2)
    return -entropy
```

**将特征分类，并提取指定的$$D_{ik}$$**

```python
def classify_category(feature, i):
    """
    :param feature: 包含该特征的各取值种类的DataFrame（含有对应的类）
    :param i:要第i个取值的信息
    :return: Di所含样本数和Dik的样本数列表（Dik为该特征的第i个取值的集合与第k个类的交集）
    """
    feature_name_i = list(feature.groupby([0]).groups.keys())[i]  # 把第i个取值的名字或数值提取出来
    data_feature_name_i = feature[feature[0] == feature_name_i]  # 把含有第i个取值的行提取出来
    number_feature_category = data_feature_name_i[-1].value_counts()  # Dik的样本数列表
    return number_feature_category
```

**计算条件熵的函数**

```python
def get_conditional_entropy(feature, data_set):
    """
    :param feature: 包含该特征的各取值种类的DataFrame（含有对应的类）
    :param data_set: 总数据集
    :return: 某个特征的条件熵
    """
    conditional_entropy = 0
    category_number_feature = feature[0].value_counts()  # 特征各种类所含的样本数
    number_feature = list(feature.groupby([0]).groups.keys()).count()  # 特征的取值种类数目
    for i in range(number_feature):
        number_feature_category = classify_category(feature, i)  # Dik的样本数列表
        factor = category_number_feature[i]/len(data_set)  # 式子太长，因子拆开写
        conditional_entropy += factor*get_entropy(number_feature_category, feature, data_set)
    return conditional_entropy
```

**选择最大信息增益的函数**

```python
def get_max_information_gain(data_information_gain):
    """
    :param data_information_gain: 包含多个信息增益的列表
    :return: 最大信息增益及其所对应的特征所在的列的编号
    """
    max_information_gain = 0
    number_best_feature = 0
    for i in range(len(data_information_gain)):
        if data_information_gain[i] > max_information_gain:
            max_information_gain = data_information_gain[i]
            number_best_feature = i
    return max_information_gain, number_best_feature
```

**寻找含样本数最多的类的值**

```python
def majorityCnt(category):
    """
    :param category: 类别列表(格式要求为Series)
    :return: 含样本数最多的类的值
    """
    category = pd.DataFrame(category.value_counts().reset_index)  # 使Series转换成含有两列的DataFrame(第一列是类，第二列是样本数)
    category.columns = range(2)  # 设置列索引

    list_number_category = category[1]  # 提取包含样本数的列
    max_number = list_number_category[0]  # 样本数最大的值
    loc_max_number = 0  # 样本数最大值所在的位置
    for i in range(len(list_number_category)):  # 寻找样本数最大值所在的位置
        if list_number_category[i] > max_number:
            max_number = list_number_category[i]
            loc_max_number = i
    return category[0][loc_max_number]
```

**划分数据集**

```python
def classify_data(data_set, i, value):
    """
    :param data_set: 要被划分的数据集
    :param i: 特征所在的列的位置
    :param value: 特征的一个取值
    :return: 划分后的数据集
    """
    new_data_set = data_set[data_set[i] == value].reset_index(drop=True)
    del new_data_set[i]
    new_data_set.columns = range(new_data_set.shape[1])  # 重新设置列索引
    return new_data_set
```

**构建ID3决策树**

大致流程：判断特殊情况--计算多个信息增益并找到最大信息增益对应的特征--划分多个数据集，同时进行递归

```python
def tree_ID3(data_set, featureName):
    """
    :param data_set: 训练集
    :param featureName: 当前所含特征名的列表
    :return:
    """
    category_list = data_set.iloc[:, -1]  # 提取类别
    if category_list.count(category_list[0]) == len(category_list):
        return category_list[0]  # 训练集中所有实例都属于同一类
    if len(featureName) == 1:  # 只有一个特征时返回训练集中实例数最大的类的值
        return majorityCnt(category_list)

    information_gain = []  # 信息增益列表
    for i in range(data_set.shape[1]-1):  # 计算每个特征的信息增益
        base_entropy = get_entropy(number_category, data_set.iloc[:, -1].value_counts(), data_set)
        conditional_entropy = get_conditional_entropy(data_set[i], data_set)
        information_gain.append(base_entropy - conditional_entropy)
    num_best_feature = get_max_information_gain(information_gain)  # 最大信息增益所对应的特征所在的列的编号
    name_best_feature = featureName[num_best_feature]  # 最大信息增益所对应的特征的名字
    del featureName[name_best_feature]

    tree = {name_best_feature: {}}  # 字典的嵌套使用

    target_feature = data_set[num_best_feature]  # 提取目标特征序列
    kind_target_feature = set(target_feature)  # 把目标特征序列的不同取值提取出来，形成集合
    for value in kind_target_feature:  # 开始划分数据集并进行递归
        data_set_new = classify_data(data_set, num_best_feature, value)
        tree[name_best_feature][value] = tree_ID3(data_set_new, featureName)
    return tree
```

