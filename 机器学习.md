# 机器学习

## 模型的评估与选择

### **引入**

背景：在m个样本中有a个样本分类错误

错误率：$$E=a/m$$，精度：$$1-a/m$$

**训练误差（经验误差）**

学习器在训练集上的误差

**泛化误差**

学习器在新样本上的误差。泛化误差不可避免

**过拟合**

机器把训练集学习得太好从而导致泛化性能下降

**欠拟合**

机器对训练样本的一般性质尚未学好

**数据集**

$$D=\{x_1,x_2,...,x_m \}$$表示包含m个示例的数据集

$$x_i=(x_{i1};x_{i2},...;x_{id})$$是d维样本空间$$\chi$$的一个向量，其中$$x_{ij}$$是$$x_{i}$$在第$$j$$个属性上的取值

**样例与标记**

样例：拥有了标记信息的示例。一般地，用$$(x_i,y_i)$$表示第$$i$$个样例，其中$$y_i \in \gamma$$是示例$$x_i$$的标记，$$\gamma$$是所有标记的集合，亦称“标记空间”或“输出空间”

**学习任务**（详见西瓜书p3）

根据欲预测的值的类型：

* 分类：欲预测的是离散值
* 回归：欲预测的是连续值

根据涉及到的类别：

* “二分类”任务：只涉及两个类别。通常一个类为“正类”，另一个为“反类”
* “多分类”任务

根据训练数据是否拥有标记信息：

* 监督学习（分类和回归是其中的代表）
* 无监督学习（聚类是其中的代表）

### 评估方法

1. 通过测试来评估学习器的泛化误差：

   * 使用测试集，但测试集与训练集的关系应该尽可能小。

   只有一个数据集时产生训练集和测试集的方法：

   1. 留出法
   2. 交叉验证法
   3. 自助法
   4. 调参与最终模型

2. 性能度量

   **回归任务中最常用得性能度量：均方误差**（见西瓜书29页）

   $$E(f;D)=\frac {1}{m}\sum\limits_{i=1}^m (f(x_i)-y_i)^2$$

   分类任务中常用的性能度量：

   * 错误率与精度

   * 查准率、查全率与F1

   * ROC与AUC

   * 代价敏感错误率与代价曲线

3. 比较检验

   * 假设检验
   * 交叉验证t检验
   * McNemar检验
   * Friedman检验与Nemenyi后续检验

4. 偏差与方差

## 线性模型

**基本形式**

$$f(x)=w^Tx+b$$，其中$$w=(w_1;w_2;...;w_d)$$，$$x=(x_1;x_2;...;x_d)$$（即含有d个属性）。目的是学得$$w$$和$$b$$

### 线性回归

#### 正规方程法：

最小二乘法：基于均方误差最小化来进行模型求解的方法

**前置设定：**

数据集：$$D=\{ (x_1,y_1)，(x_2，y_2),...,(x_n,y_n) \}$$

$$X=(x_1,x_2,...,x_n)^T=\begin{pmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{pmatrix}$$，$$Y=\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} $$，其中$$x_i \in R^p$$，$$y_i \in R$$，$$w=\begin{pmatrix}   w_1 \\ w_2 \\ \vdots \\ w_n\end{pmatrix}$$

设$$f(w)=w^Tx$$

推导：
$$
\begin{aligned}
L(w)&=\sum\limits_{i=1}^m||w^Tx_i-y_i||^2 \qquad 最小二乘法\\&=\sum\limits_{i=1}^m(w^Tx_i-y_i)^2 \\&=[w^Tx_1-y_1 \quad w^Tx_2-y_2 \quad...\quad w^Tx_n-y_n] \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=([w^Tx_1 \quad w^Tx_2 \quad...\quad w^Tx_n]-[y_1 \quad y_2 \quad...\quad y_n]) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^T[x_1 \quad x_2 \quad...\quad x_n]-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T)(Xw-Y) \qquad 从第三步可以看出右边的矩阵是左边矩阵的逆置 \\&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY \qquad 顺序不能写错\\&=w^TX^TXw-2w^TX^TY+Y^TY \qquad上一步的中间两项互为逆置，因此结果相同（都为同一个数），可以合并
\end{aligned}
$$
所以，$$\hat{w}=arg\min L(w)$$.

> arg   是变元（即自变量argument）的英文缩写。
> arg min 就是使后面这个式子达到最小值时的变量的取值
> arg max 就是使后面这个式子达到最大值时的变量的取值

求最小值的方法是对$$w$$求导，并令式子为0，求出此时的$$w$$：
$$
\begin{aligned}
&\frac{\partial L(w)}{\partial w}=2X^TXw-2X^TY=0 \\&\therefore X^TXw=X^TY \\&\therefore w=(X^TX)^{-1}X^TY
\end{aligned}
$$
