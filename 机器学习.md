# 机器学习

## 模型的评估与选择

### **引入**

背景：在m个样本中有a个样本分类错误

错误率：$$E=a/m$$，精度：$$1-a/m$$

**训练误差（经验误差）**

学习器在训练集上的误差

**泛化误差**

学习器在新样本上的误差。泛化误差不可避免

**过拟合**

机器把训练集学习得太好从而导致泛化性能下降

**欠拟合**

机器对训练样本的一般性质尚未学好

**数据集**

$$D=\{x_1,x_2,...,x_m \}$$表示包含m个示例的数据集

$$x_i=(x_{i1};x_{i2},...;x_{id})$$是d维样本空间$$\chi$$的一个向量，其中$$x_{ij}$$是$$x_{i}$$在第$$j$$个属性上的取值

**样例与标记**

样例：拥有了标记信息的示例。一般地，用$$(x_i,y_i)$$表示第$$i$$个样例，其中$$y_i \in \gamma$$是示例$$x_i$$的标记，$$\gamma$$是所有标记的集合，亦称“标记空间”或“输出空间”

**学习任务**（详见西瓜书p3）

根据欲预测的值的类型：

* 分类：欲预测的是离散值
* 回归：欲预测的是连续值

根据涉及到的类别：

* “二分类”任务：只涉及两个类别。通常一个类为“正类”，另一个为“反类”
* “多分类”任务

根据训练数据是否拥有标记信息：

* 监督学习（分类和回归是其中的代表）
* 无监督学习（聚类是其中的代表）

### 评估方法

1. 通过测试来评估学习器的泛化误差：

   * 使用测试集，但测试集与训练集的关系应该尽可能小。

   只有一个数据集时产生训练集和测试集的方法：

   1. 留出法

   2. **交叉验证法（$$k$$折交叉验证）：**

      * [交叉验证法与留出法及其python实现](https://blog.csdn.net/weixin_43216017/article/details/86692358?ops_request_misc=%7B%22request%5Fid%22%3A%22164282550316780271560942%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164282550316780271560942&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-86692358.first_rank_v2_pc_rank_v29&utm_term=留一法交叉验证python&spm=1018.2226.3001.4187)

      * 将数据集$$D$$划分成$$k$$个大小相似的互斥子集，每次用$$k-1$$个子集的并集作为训练集，余下的子集是测试集，这样就可以进行$$k$$次训练与测试，最终返回的是这$$k$$个测试结果的均值
      * 留一法（数据集较大时慎用）：令$$k=样本数$$

   3. 自助法

   4. 调参与最终模型

2. 性能度量

   **回归任务中最常用得性能度量：均方误差**（$$MSE$$）（见西瓜书29页）

   $$E(f;D)=\frac {1}{m}\sum\limits_{i=1}^m (f(x_i)-y_i)^2$$

   **分类任务中常用的性能度量：**

   * 错误率与精度

   * 查准率、查全率与F1

   * ROC与AUC

   * 代价敏感错误率与代价曲线

3. 比较检验

   * 假设检验
   * 交叉验证t检验
   * McNemar检验
   * Friedman检验与Nemenyi后续检验

4. 偏差与方差

## 线性模型

**基本形式**

$$f(x)=w^Tx+b$$，其中$$w=(w_1;w_2;...;w_d)$$，$$x=(x_1;x_2;...;x_d)$$（即含有d个属性）。目的是学得$$w$$和$$b$$

### 线性回归

#### 正规方程法：

**能直接求出解析解，因此不用训练多次！**

最小二乘法：基于均方误差最小化来进行模型求解的方法

**前置设定：**

数据集：$$D=\{ (x_1,y_1)，(x_2，y_2),...,(x_n,y_n) \}$$

$$X=(x_1,x_2,...,x_n)^T=\begin{pmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{pmatrix}$$，$$Y=\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} $$，其中$$x_i \in R^p$$，$$y_i \in R$$，$$w=\begin{pmatrix}   w_1 \\ w_2 \\ \vdots \\ w_n\end{pmatrix}$$

设$$f(w)=w^Tx$$

推导：
$$
\begin{aligned}
L(w)&=\sum\limits_{i=1}^m||w^Tx_i-y_i||^2 \qquad 最小二乘法\\&=\sum\limits_{i=1}^m(w^Tx_i-y_i)^2 \\&=[w^Tx_1-y_1 \quad w^Tx_2-y_2 \quad...\quad w^Tx_n-y_n] \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=([w^Tx_1 \quad w^Tx_2 \quad...\quad w^Tx_n]-[y_1 \quad y_2 \quad...\quad y_n]) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^T[x_1 \quad x_2 \quad...\quad x_n]-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T) \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_n-y_n \end{pmatrix} \\&=(w^TX^T-Y^T)(Xw-Y) \qquad 从第三步可以看出右边的矩阵是左边矩阵的逆置 \\&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY \qquad 顺序不能写错\\&=w^TX^TXw-2w^TX^TY+Y^TY \qquad上一步的中间两项互为逆置，因此结果相同（都为同一个数），可以合并
\end{aligned}
$$
所以，$$\hat{w}=arg\min L(w)$$.

> arg   是变元（即自变量argument）的英文缩写。
> arg min 就是使后面这个式子达到最小值时的变量的取值
> arg max 就是使后面这个式子达到最大值时的变量的取值

求最小值的方法是对$$w$$求导，并令式子为0，求出此时的$$w$$：
$$
\begin{aligned}
&\frac{\partial L(w)}{\partial w}=2X^TXw-2X^TY=0 \\&\therefore X^TXw=X^TY \\&\therefore w=(X^TX)^{-1}X^TY
\end{aligned}
$$

##### **代码实现：**

**主要步骤：**导入数据集-查看数据集类型-转换数据集为多维矩阵类型（如需转置则转置）-定义正规方程的函数并经此将$$w$$求出-分别训练自己的模型与sklearn的模型-比较二者最终的损失函数值

**sklearn入门：**

[非常详细的sklearn介绍](https://blog.csdn.net/algorithmPro/article/details/103045824?ops_request_misc=%7B%22request%5Fid%22%3A%22164272427916780274136815%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164272427916780274136815&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-103045824.first_rank_v2_pc_rank_v29&utm_term=sklearn&spm=1018.2226.3001.4187)

[极简Scikit-Learn入门](https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2648929856&idx=1&sn=0a6ce240f46bea7d74f2a850625de832&chksm=8794e46ab0e36d7c449579513e69fbbc957ac61eac10c6f863ed4a860debd3dca1ed0ba442c2&token=2004915986&lang=en_US#rd)

[机器学习神器：sklearn的快速使用](https://juejin.cn/post/6844903632576446478)

**Linear Regression中的参数及功能：**

[线性模型之Linear Regression_walk_power](https://blog.csdn.net/walk_power/article/details/82924363?ops_request_misc=%7B%22request%5Fid%22%3A%22164278046516780261964167%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164278046516780261964167&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-82924363.first_rank_v2_pc_rank_v29&utm_term=linearregression.fit函数&spm=1018.2226.3001.4187)

**糖尿病数据集介绍：**

官方（英文）：[7.1. Toy datasets — scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)

中文：[sklearn数据集之“糖尿病病人体检数据集”](https://blog.csdn.net/Zee_Chao/article/details/95884401?ops_request_misc=%7B%22request%5Fid%22%3A%22164275178816780357286281%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164275178816780357286281&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-95884401.first_rank_v2_pc_rank_v29&utm_term=sklearn糖尿病数据集&spm=1018.2226.3001.4187)

**矩阵的运算：**

[矩阵的创建与运算](https://blog.csdn.net/m0_37759590/article/details/105160374?ops_request_misc=%7B%22request%5Fid%22%3A%22164274088716780271983059%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164274088716780271983059&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105160374.first_rank_v2_pc_rank_v29&utm_term=python+mat()&spm=1018.2226.3001.4187)

**损失函数相关知识：**

[均方误差，log loss和交叉熵](https://blog.csdn.net/weixin_38410551/article/details/104973011?ops_request_misc=%7B%22request%5Fid%22%3A%22164277875816780271580985%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164277875816780271580985&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-104973011.first_rank_v2_pc_rank_v29&utm_term=均方误差损失函数&spm=1018.2226.3001.4187)

**pytorch的运用与torch-cpu的安装：**

[深度学习环境配置5——windows下的torch-cpu=1.2.0环境配置](https://blog.csdn.net/weixin_44791964/article/details/120655098)

`HTTPError: HTTP 000 CONNECTION FAILED`错误的解决办法：[在终端通过conda指令创建虚拟python环境时，出现CondaHTTPError: HTTP 000 CONNECTION FAILED等错误](https://blog.csdn.net/kimpa0723/article/details/118912484?ops_request_misc=%7B%22request%5Fid%22%3A%22164286290616780255269742%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=164286290616780255269742&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-118912484.first_rank_v2_pc_rank_v29&utm_term=CondaHTTPError%3A&spm=1018.2226.3001.4187)

[PyTorch中文文档 ](https://pytorch-cn.readthedocs.io/zh/latest/)

**`LeaveOneOut()`相关：**

官方文档：[sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leaveoneout#sklearn.model_selection.LeaveOneOut)

**在矩阵（多维数组）前加上一列：**

[np.hstack 用法](https://blog.csdn.net/G66565906/article/details/84142034?ops_request_misc=%7B%22request%5Fid%22%3A%22164292971316780269822279%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164292971316780269822279&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-84142034.first_rank_v2_pc_rank_v29&utm_term=np.hstack&spm=1018.2226.3001.4187)

[Python中的numpy.ones()](https://blog.csdn.net/cunchi4221/article/details/107471968?ops_request_misc=%7B%22request%5Fid%22%3A%22164292924816781683974723%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164292924816781683974723&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107471968.first_rank_v2_pc_rank_v29&utm_term=np.ones&spm=1018.2226.3001.4187)

**设定与说明：**

该数据集的类型为sklearn.utils.Bunch（本质是字典），详见[sklearn.utils.Bunch的属性](https://blog.csdn.net/lly1122334/article/details/88741971?ops_request_misc=%7B%22request%5Fid%22%3A%22164275423516780264093759%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164275423516780264093759&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-88741971.first_rank_v2_pc_rank_v29&utm_term=sklearn.utils.Bunch&spm=1018.2226.3001.4187)

用`data`来装数据集

sklearn中的留一法函数的运用（基本仿照官方示例打的）：

*注：`split()`返回的只是分组之后的组的索引值*

python里对向量进行次方的操作是对每一个元素单独进行次方，而不是点乘。

* 求预测值从而求$$MSE$$用的是向量的方法直接求出一个整体：`(np.array(X_train*wT.T-Y_train)**2).sum()/441`（不先转成矩阵再平方会报错，因为是非方阵）

注：该方法用$$Xw$$而不是$$w^Tx$$

```python
loo = LeaveOneOut()
for train_index, test_index in loo.split(X):
    print("train_index:", train_index, "test_index:", test_index)
    X_train = X[train_index]
    Y_train = Y[train_index]
# 训练集的录入
    X_test = X[test_index]
    Y_test = Y[test_index]
# 测试集的录入
```

* 需在$$X$$的前面加上一列$$x_0=1$$，不然$$MSE$$的值会很大

```python
X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
```

* sklearn中的线性回归函数的调用：

```python
lr_fun = LinearRegression()
lr_fun.fit(np.array(X_train), np.array(Y_train))
# X与Y的输入
y_pre_sk = lr_fun.predict(np.array(X_train))
print("sk_loss:", (np.array(y_pre_sk-Y_train)**2).sum()/441)
```

全部代码：

```python
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import LeaveOneOut
# 导入留一法函数
data = datasets.load_diabetes()
# 导入糖尿病数据集


def linear_regression(X, Y):
    """
    求wT
    """
    XT = X.T
    XTX = XT*X
    w = XTX.I*XT*Y
    return w


X = data.data
X = np.hstack((np.ones((len(X), 1)), X))
X = np.mat(X)
# 这里不用转置

Y = data.target
Y = np.mat(Y).T

loo = LeaveOneOut()
for train_index, test_index in loo.split(X):
    X_train = X[train_index]
    Y_train = Y[train_index]
    X_test = X[test_index]
    Y_test = Y[test_index]

w = linear_regression(X_train, Y_train)
print("loss:", (np.array(X_train*w-Y_train)**2).sum()/441)

lr_fun = LinearRegression()
lr_fun.fit(np.array(X_train), np.array(Y_train))
# X与Y的输入
y_pre_sk = lr_fun.predict(np.array(X_train))
print("sk_loss:", (np.array(y_pre_sk-Y_train)**2).sum()/441)
```

